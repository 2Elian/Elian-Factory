#!/usr/bin/env python
# -*- coding: utf-8 -*-
# @Time    : 2025/4/30 17:31
# @Author  : lizimo@nuist.edu.cn
# @File    : combin_weight.py
# @Description: åˆå¹¶æºæ¨¡å‹æƒé‡å’Œå¾®è°ƒåçš„adapoteræƒé‡
from peft import PeftModel
from transformers import AutoModelForCausalLM
import torch
import os
from modelscope import AutoTokenizer
import shutil
import gc

def copy_files_not_in_B(A_path, B_path):
    """
    Copies files from directory A to directory B if they exist in A but not in B.

    :param A_path: Path to the source directory (A).
    :param B_path: Path to the destination directory (B).
    """
    # ä¿è¯è·¯å¾„å­˜åœ¨
    if not os.path.exists(A_path):
        raise FileNotFoundError(f"The directory {A_path} does not exist.")
    if not os.path.exists(B_path):
        os.makedirs(B_path)

    # è·å–è·¯å¾„Aä¸­æ‰€æœ‰éæƒé‡æ–‡ä»¶
    files_in_A = os.listdir(A_path)
    files_in_A = set([file for file in files_in_A if not (".bin" in file or "safetensors" in file)])
    # List all files in directory B
    files_in_B = set(os.listdir(B_path))

    # æ‰¾åˆ°æ‰€æœ‰Aä¸­å­˜åœ¨ä½†Bä¸­ä¸å­˜åœ¨çš„æ–‡ä»¶
    files_to_copy = files_in_A - files_in_B

    # å°†æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹å¤åˆ¶åˆ°Bè·¯å¾„ä¸‹
    for file in files_to_copy:
        src_path = os.path.join(A_path, file)
        dst_path = os.path.join(B_path, file)

        if os.path.isdir(src_path):
            # å¤åˆ¶ç›®å½•åŠå…¶å†…å®¹
            shutil.copytree(src_path, dst_path)
        else:
            # å¤åˆ¶æ–‡ä»¶
            shutil.copy2(src_path, dst_path)

def merge_lora_to_base_model(model_name_or_path, adapter_name_or_path, save_path):
    # æ¸…ç©ºGPUç¼“å­˜
    torch.cuda.empty_cache()
    gc.collect()
    
    try:
        # å¦‚æœæ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼Œå°±åˆ›å»º
        if not os.path.exists(save_path):
            os.makedirs(save_path)
            
        # ç¡®ä¿ä½¿ç”¨CPUåŠ è½½æ¨¡å‹ä»¥èŠ‚çœæ˜¾å­˜
        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)
        
        print("â³ å¼€å§‹åŠ è½½åŸºç¡€æ¨¡å‹...")
        model = AutoModelForCausalLM.from_pretrained(
            model_name_or_path,
            trust_remote_code=True,
            low_cpu_mem_usage=True,
            torch_dtype=torch.float16,
            device_map="auto"  # è‡ªåŠ¨é€‰æ‹©å¯ç”¨è®¾å¤‡
        )
        
        print("â³ åŠ è½½é€‚é…å™¨æƒé‡...")
        model = PeftModel.from_pretrained(model, adapter_name_or_path, device_map="auto")
        
        print("â³ åˆå¹¶æƒé‡ä¸­...")
        merged_model = model.merge_and_unload()
        
        print("â³ ä¿å­˜åˆå¹¶åæ¨¡å‹...")
        tokenizer.save_pretrained(save_path)
        merged_model.save_pretrained(save_path, safe_serialization=True)  # ä½¿ç”¨å®‰å…¨åºåˆ—åŒ–
        
        copy_files_not_in_B(model_name_or_path, save_path)
        print("âœ… åˆå¹¶å®Œæ¯•!@Elian")
        
    except Exception as e:
        print(f"âŒ åˆå¹¶è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        # å°è¯•ä½¿ç”¨CPUå›é€€
        try:
            print("ğŸ”„ å°è¯•ä½¿ç”¨CPUå›é€€æ–¹æ¡ˆ...")
            model = AutoModelForCausalLM.from_pretrained(
                model_name_or_path,
                trust_remote_code=True,
                low_cpu_mem_usage=True,
                torch_dtype=torch.float16,
                device_map="cpu"
            )
            model = PeftModel.from_pretrained(model, adapter_name_or_path, device_map="cpu")
            merged_model = model.merge_and_unload()
            tokenizer.save_pretrained(save_path)
            merged_model.save_pretrained(save_path, safe_serialization=True)
            copy_files_not_in_B(model_name_or_path, save_path)
            print("âœ… CPUå›é€€æ–¹æ¡ˆåˆå¹¶æˆåŠŸ!@Elian")
        except Exception as cpu_e:
            print(f"âŒ CPUå›é€€æ–¹æ¡ˆå¤±è´¥: {str(cpu_e)}")
    finally:
        # æœ€ç»ˆæ¸…ç†
        del model
        del merged_model
        torch.cuda.empty_cache()
        gc.collect()

if __name__ == '__main__':
    model_name_or_path = "./ckpt/deepseek14"
    adapter_name_or_path = "E:\lottery\Elian-Factory-main\llm\output\your_t123ask\lora"
    save_path = "E:\lottery\Elian-Factory-main\llm\output\your_t123ask\lora\weight"
    merge_lora_to_base_model(model_name_or_path,adapter_name_or_path,save_path)

